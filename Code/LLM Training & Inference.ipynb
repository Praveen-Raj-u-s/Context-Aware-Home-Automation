{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Function Prediction Model"
      ],
      "metadata": {
        "id": "Z3Wif7Mzrei8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training:"
      ],
      "metadata": {
        "id": "J8UcwPKyseLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pandas as pd\n",
        "\n",
        "#For DataHandling\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "  def __init__(self, examples, tokenizer, max_length=128):\n",
        "    self.examples = examples\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_length = max_length\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.examples)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    example = self.examples[idx]\n",
        "    text = example[\"text\"]\n",
        "    label = torch.tensor(example[\"label\"], dtype=torch.long)\n",
        "\n",
        "\n",
        "    inputs = self.tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        max_length=self.max_length,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": inputs[\"input_ids\"].squeeze(),\n",
        "        \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n",
        "        \"label\": label,\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "excel_file_path = \"/content/dataset_multiclass_3.xlsx\"\n",
        "\n",
        "df = pd.read_excel(excel_file_path)\n",
        "\n",
        "examples = df[[\"text\", \"label\"]].to_dict(\"records\")\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "num_classes = len(df[\"label\"].unique())\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_classes)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "custom_dataset = CustomDataset(examples, tokenizer)\n",
        "dataloader = DataLoader(custom_dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "  model.train()\n",
        "  for batch in dataloader:\n",
        "\n",
        "    inputs = batch[\"input_ids\"]\n",
        "    attention_mask = batch[\"attention_mask\"]\n",
        "    labels = batch[\"label\"]\n",
        "\n",
        "    #Fpass\n",
        "    outputs = model(input_ids=inputs, attention_mask=attention_mask, labels=labels)\n",
        "    loss = outputs.loss\n",
        "\n",
        "    #Bpass\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "  print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
        "\n",
        "\n",
        "model.save_pretrained(\"save/model_multiclass\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRFjsjIqgL2K",
        "outputId": "6cb0f769-2c24-43e2-9198-6c18e4a6f43b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.7475414276123047\n",
            "Epoch 2, Loss: 1.6825226545333862\n",
            "Epoch 3, Loss: 1.2501829862594604\n",
            "Epoch 4, Loss: 1.183195948600769\n",
            "Epoch 5, Loss: 1.2933166027069092\n",
            "Input Prompt: song on\n",
            "Predicted Function Call: play_music();\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference:"
      ],
      "metadata": {
        "id": "VTpgW88WsgKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Loading the trained model\n",
        "model = BertForSequenceClassification.from_pretrained(\"save/model_multiclass\")\n",
        "\n",
        "model.eval()\n",
        "\n",
        "input_prompts = [\"song on\"]\n",
        "\n",
        "#tokanizing input with dynamic padding\n",
        "tokenized_inputs = tokenizer(input_prompts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "  logits = model(**tokenized_inputs).logits\n",
        "\n",
        "\n",
        "probs = torch.nn.functional.softmax(logits, dim=1)\n",
        "\n",
        "predicted_classes = torch.argmax(probs, dim=1)\n",
        "\n",
        "class_to_function_mapping = {\n",
        "    0: \"time_date_wolfram();\",\n",
        "    1: \"play_music();\",\n",
        "    2: \"lights();\",\n",
        "    3: \"increase_volume();\",\n",
        "    4: \"decrease_volume();\",\n",
        "    5: \"increase_brightness();\",\n",
        "    6: \"decrease_brightness()\"\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "predicted_function_calls = [class_to_function_mapping[class_idx.item()] for class_idx in predicted_classes]\n",
        "\n",
        "# Print the results\n",
        "for input_prompt, function_call in zip(input_prompts, predicted_function_calls):\n",
        "\n",
        "  print(f\"Input Prompt: {input_prompt}\")\n",
        "  print(f\"Predicted Function Call: {function_call}\")\n",
        "  print()\n"
      ],
      "metadata": {
        "id": "-doIviyJi7vu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dual Model - Funcion Call Prediction with Parameter Prediction: Training"
      ],
      "metadata": {
        "id": "FY8pj9WXtGLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pandas as pd\n",
        "\n",
        "class FunctionCallModel(BertForSequenceClassification):\n",
        "    def __init__(self, num_classes):\n",
        "        super(FunctionCallModel, self).__init__(from_pretrained=\"bert-base-uncased\", num_labels=num_classes)\n",
        "\n",
        "\n",
        "#Function-call Prediction model\n",
        "class FunctionCallModel(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, num_classes):\n",
        "    super(FunctionCallModel, self).__init__()\n",
        "    self.model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_classes)\n",
        "\n",
        "  def forward(self, inputs, attention_mask):\n",
        "\n",
        "    outputs = self.model(input_ids=inputs, attention_mask=attention_mask, labels=None)\n",
        "    logits = outputs.logits\n",
        "    return logits\n",
        "\n",
        "\n",
        "#Function-parameter Prediction Model\n",
        "class FunctionParameterModel(torch.nn.Module):\n",
        "  def __init__(self, num_classes):\n",
        "    super(FunctionParameterModel, self).__init__()\n",
        "    self.model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_classes)\n",
        "\n",
        "  def forward(self, inputs, attention_mask):\n",
        "    outputs = self.model(input_ids=inputs, attention_mask=attention_mask, labels=None)\n",
        "    logits = outputs.logits\n",
        "    return logits\n",
        "\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "  def __init__(self, examples, tokenizer, max_length=128):\n",
        "    self.examples = examples\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_length = max_length\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.examples)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    example = self.examples[idx]\n",
        "    text = str(example[\"text\"])\n",
        "    label = torch.tensor(example[\"label\"], dtype=torch.long)\n",
        "\n",
        "\n",
        "    inputs = self.tokenizer(text, return_tensors=\"pt\", max_length=self.max_length, padding=\"max_length\", truncation=True,)\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": inputs[\"input_ids\"].squeeze(),\n",
        "        \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n",
        "        \"label\": label,\n",
        "    }\n",
        "\n",
        "excel_file_path_function = \"/content/dataset_multiclass_3.xlsx\"\n",
        "excel_file_path_parameter = \"/content/parameter_dataset_3.xlsx\"\n",
        "\n",
        "df_function = pd.read_excel(excel_file_path_function)\n",
        "df_parameter = pd.read_excel(excel_file_path_parameter)\n",
        "\n",
        "examples_function = df_function[[\"text\", \"label\"]].to_dict(\"records\")\n",
        "examples_parameter = df_parameter[[\"text\", \"label\"]].to_dict(\"records\")\n",
        "\n",
        "tokenizer_function = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer_parameter = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "num_classes_function = len(df_function[\"label\"].unique())\n",
        "num_classes_parameter = len(df_parameter[\"label\"].unique())\n",
        "model_f = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_classes_function)\n",
        "model_p = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_classes_parameter)\n",
        "model_function = FunctionCallModel(num_classes_function)\n",
        "model_parameter = FunctionParameterModel(num_classes_parameter)\n",
        "\n",
        "optimizer_function = AdamW(model_function.parameters(), lr=2e-5)\n",
        "optimizer_parameter = AdamW(model_parameter.parameters(), lr=2e-5)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "#Set up DataLoaders\n",
        "custom_dataset_function = CustomDataset(examples_function, tokenizer_function)\n",
        "dataloader_function = DataLoader(custom_dataset_function, batch_size=8, shuffle=True)\n",
        "\n",
        "custom_dataset_parameter = CustomDataset(examples_parameter, tokenizer_parameter)\n",
        "dataloader_parameter = DataLoader(custom_dataset_parameter, batch_size=8, shuffle=True)\n",
        "\n",
        "#Training for Function-call Prediction Model\n",
        "print(\"Training Function Call Prediction Model:\")\n",
        "print(\" \")\n",
        "\n",
        "num_epochs_function = 10\n",
        "for epoch in range(num_epochs_function):\n",
        "\n",
        "  model_function.train()\n",
        "  for batch in dataloader_function:\n",
        "\n",
        "    inputs = batch[\"input_ids\"]\n",
        "    attention_mask = batch[\"attention_mask\"]\n",
        "    labels = batch[\"label\"]\n",
        "\n",
        "\n",
        "    outputs = model_function(inputs=inputs, attention_mask=attention_mask)\n",
        "    loss = loss_fn(outputs, labels)\n",
        "\n",
        "    optimizer_function.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer_function.step()\n",
        "\n",
        "  print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
        "\n",
        "\n",
        "model_f.save_pretrained(\"save/model_multiclass\")\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "torch.save({\n",
        "    'model_state_dict': model_function.state_dict(),\n",
        "    'optimizer_state_dict': optimizer_function.state_dict(),\n",
        "    'loss': loss_fn,\n",
        "}, \"save/model_multiclass.pth\")\n",
        "\"\"\"\n",
        "\n",
        "print(\"---------------------------------------------------------------\")\n",
        "print(\"Training Function Parameter Prediction Model:\")\n",
        "\n",
        "\n",
        "num_epochs_parameter = 10\n",
        "for epoch in range(num_epochs_parameter):\n",
        "\n",
        "  model_parameter.train()\n",
        "  for batch in dataloader_parameter:\n",
        "\n",
        "    inputs = batch[\"input_ids\"]\n",
        "    attention_mask = batch[\"attention_mask\"]\n",
        "    labels = batch[\"label\"]\n",
        "\n",
        "\n",
        "    outputs = model_parameter(inputs=inputs, attention_mask=attention_mask)\n",
        "    loss = loss_fn(outputs, labels)\n",
        "\n",
        "\n",
        "    optimizer_parameter.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer_parameter.step()\n",
        "\n",
        "\n",
        "  print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
        "\n",
        "\n",
        "model_p.save_pretrained(\"save/model_parameters\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HY9qxiAOC0t",
        "outputId": "f4c7ed75-2533-4063-e359-01f9bc63995d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Function Call Prediction Model:\n",
            " \n",
            "Epoch 1, Loss: 1.9984384775161743\n",
            "Epoch 2, Loss: 2.0864670276641846\n",
            "Epoch 3, Loss: 1.730629801750183\n",
            "Epoch 4, Loss: 1.559712290763855\n",
            "Epoch 5, Loss: 1.8500081300735474\n",
            "Epoch 6, Loss: 1.650430679321289\n",
            "Epoch 7, Loss: 1.1223037242889404\n",
            "Epoch 8, Loss: 0.7647712826728821\n",
            "Epoch 9, Loss: 1.1426070928573608\n",
            "Epoch 10, Loss: 0.6659547686576843\n",
            "---------------------------------------------------------------\n",
            "Training Function Parameter Prediction Model:\n",
            "Epoch 1, Loss: 1.377889633178711\n",
            "Epoch 2, Loss: 1.0356718301773071\n",
            "Epoch 3, Loss: 0.8999375104904175\n",
            "Epoch 4, Loss: 0.710089921951294\n",
            "Epoch 5, Loss: 0.5664712190628052\n",
            "Epoch 6, Loss: 0.4347323477268219\n",
            "Epoch 7, Loss: 0.2845042645931244\n",
            "Epoch 8, Loss: 0.1854538470506668\n",
            "Epoch 9, Loss: 0.6032761931419373\n",
            "Epoch 10, Loss: 0.12956838309764862\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_function = BertForSequenceClassification.from_pretrained(\"save/model_multiclass\")\n",
        "model_parameter = BertForSequenceClassification.from_pretrained(\"save/model_parameters\")\n",
        "\n",
        "\n",
        "model_function.eval()\n",
        "model_parameter.eval()\n",
        "\n",
        "\n",
        "input_prompts = [\"on the light\"]\n",
        "\n",
        "\n",
        "tokenized_inputs = tokenizer_function(input_prompts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "  logits_function = model_function(**tokenized_inputs).logits\n",
        "\n",
        "probs_function = torch.nn.functional.softmax(logits_function, dim=1)\n",
        "\n",
        "predicted_classes_function = torch.argmax(probs_function, dim=1)\n",
        "\n",
        "class_to_function_mapping = {\n",
        "    0: \"increase_volume\",\n",
        "    1: \"decrease_volume\",\n",
        "    2: \"lights_on\",\n",
        "    3: \"lights_off\",\n",
        "    4: \"increase_brightness\",\n",
        "    5: \"decrease_brightness\"\n",
        "}\n",
        "\n",
        "predicted_function_calls = [class_to_function_mapping[class_idx.item()] for class_idx in predicted_classes_function]\n",
        "\n",
        "with torch.no_grad():\n",
        "  logits_parameter = model_parameter(**tokenized_inputs).logits\n",
        "\n",
        "probs_parameter = torch.nn.functional.softmax(logits_parameter, dim=1)\n",
        "\n",
        "predicted_classes_parameter = torch.argmax(probs_parameter, dim=1)\n",
        "\n",
        "class_to_parameter_mapping = {\n",
        "    0: \"10\",\n",
        "    1: \"15\",\n",
        "    2: \"on\",\n",
        "    3: \"off\",\n",
        "}\n",
        "\n",
        "predicted_parameters = [class_to_parameter_mapping[class_idx.item()] for class_idx in predicted_classes_parameter]\n",
        "\n",
        "for input_prompt, function_call, parameter in zip(input_prompts, predicted_function_calls, predicted_parameters):\n",
        "  print(f\"Input Prompt: {input_prompt}\")\n",
        "  print(f\"Predicted Function Call: {function_call}\")\n",
        "  print(f\"Predicted Parameter: {parameter}\")\n",
        "  print()\n"
      ],
      "metadata": {
        "id": "i_lCT847E1Ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b127I4PMZ3AK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}